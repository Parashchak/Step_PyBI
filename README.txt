Step Project


База даних IKEA

1. Завантажте цей набір даних IKEA.

2. Виконайте дослідницький аналіз набору даних, включаючи описову статистику та візуалізації(за бажанням). 
Опишіть результати. 

3. На основі EDA та вашого здорового глузду виберіть дві гіпотези, які ви хочете перевірити/проаналізувати. 
Для кожної гіпотези перерахуйте нульову гіпотезу та інші можливі альтернативні гіпотези, розробіть тести для їх розрізнення та виконайте їх. 
Опишіть результати.

4. Навчіть модель передбачати ціну меблів. Зазначте, які стовпці не слід включати до моделі і чому. 
Створіть конвеєр перехресної перевірки для навчання та оцінки моделі, включаючи (за необхідності) такі кроки, як заповнення пропущених значень та нормалізація. 
Запропонуйте методи покращення продуктивності моделі. Опишіть результати.


Рекомендації по виконанню


Корисні посилання:

https://www.youtube.com/watch?v=hGKuTfhNBoA

https://www.youtube.com/watch?v=KW7Z7zgeDDE

https://www.youtube.com/watch?v=DuOy_7vxk08

https://www.youtube.com/watch?v=_os_ToGLkV0

https://www.youtube.com/watch?v=Mr5_wmlUFuk

https://www.youtube.com/watch?v=R-wnelCGS1I 

https://dan-it.com.ua/uk/students-projects/business-intelligence-ikea/

https://dan-it.com.ua/uk/students-projects/bi_final_project/



Перед тим, як робити степ-проект, подивіться це.


Частина перша: очистка даних. 
Тут має бути очистка як мінімум по item_id (дивіться вкладений приклад Step_project_example.py), але не тільки. 
Може бути аналіз викидів, аналіз даних в конкретних колонках. Треба відсіяти сутнісні дублікати - тобто такі рядки, 
які формально, з першого погляду не є дублікатами, але є такими по суті. 

Також можна відсіяти ті дані, які деформують модель даних і, відповідно, деформують наше уявлення про прогноз ціни. 
Але тут треба не переборщити (в прикладі Step_project_example.py якраз є перебор: в результаті того, 
що з моделі даних були викинуті викиди, на крос-валідації ми отримали перенавчання).

В першій частині степ-проекту треба підійти творчо. 
Чимало ідей можна почерпнути з відео (див. посилання) та прикладу Step_project_example.py. 



Частина друга: описова статистика. 

Ця частина має бути: 1) об'ємною 2) з великою кількістю візуалізацій. 

Те, як це зроблено в прикладі Step_project_example.py, - далеко не межа досконалості. 
Наприклад, в прикладі відсутня візуалізація по співвідношенням продається / не продається онлайн, old price / price і т.д. 



Частина третя: має 2 гіпотези та 2 - 3 РІЗНИХ теста до кожної з них (це добре показано в прикладі Step_project_example.py). 
Гіпотези будь-які, на Ваш смак, але бажано, щоб вони були нетривіальними, творчими, яких ще не висували інші студенти.



Частина четверта: машинне навчання. 

Ця частина має бути: 1) об'ємною 2) з великою кількістю візуалізацій. 

При загрузці даних має бути перевірений status_code (див. приклад). 
Препроцесінг даних обов'язково повинен мати Pipeline.

Обов'язково має бути як мінімум 3 різних моделі даних. 
Бажано аналізувати щонайменше 3 показники: RMSE, MSE, r2_score 
(щодо цього показника та думок щодо його важливості дивіться файл machine_learning_4_with r2_score.py у вкладеному архіві Python_step). 
Зверніть увагу, що при створенні моделей даних кількість рядків коду порівняно з прикладом можна значно скоротити, використавши функцію. 

Можна також використати такий інструмент, як pycaret, дивіться https://pycaret.org/. Приклад використання цього інструменту:

https://colab.research.google.com/drive/1FmHeUUFLEdLaTJwtrCLWFeOecGKeMb0q?usp=sharing 
(я не впевнений, чи це посилання буде дієвим, коли Ви його будете відкривати). 
При використанні pycaret зверніть увагу,
що серед моделей відсутні найкращі – RandomForestRegressor, BaggingRegressor, xgb.XGBRegressor, BlendingRegressor, HistGradientBoostingRegressor, 
виконуються лише ті моделі, які працюють швидко, 
тому доведеться скомбінувати використання цього інструмента та смаостійне створення найбільш ефективних моделей даних. 
Тому я би надав перевагу Вашому самостійному створенню та опрацюванню моделей. 

Порівняння моделей має бути зображене відповідною візуалізацією.

Як мінімум до однієї з моделей має бути застосований такий інструмент, як GridSearchCV, і відповідно, 
модель даних має бути виконана ше раз, але вже зі знайденими найкращими гіперпараметрами. 
Для відпрацювання інструменту GridSearchCV я рекомендую вибрати модель DecisionTreeRegressor. 

В кінці має бути крос-валідація як мінімум найкращої моделі даних (не завадить зробити крос-валідацію декількох моделей даних), візуалізація крос-валідації.

Приклад виконання четвертої частини завдання доволі добре представлений в вкладених файлах архіву Python_step, крім згаданого мною нюансу: 
при створенні моделей даних і порівнянні RMSE, MSE, r2_score краще використати функцію, 
як це виконано в https://colab.research.google.com/drive/1rKKygjtkzJnfkVK1CPGQa6yBot0-bMBt?resourcekey=0-meGMmt5x52rjpQN7IY83Kg 
(я не впевнений, чи це посилання буде дієвим, коли Ви його будете відкривати). 
Якщо не відкриється, можете також подивитися вкладений приклад project_ikea.ipynb.